# OncoCITE with Qwen-Agent Framework

This README describes the new Qwen-Agent implementation of the OncoCITE 18-agent system, which provides an open-source alternative to the OpenAI Agents SDK implementation.

## üéØ Quick Start

### Installation

```bash
# Clone the repository (if not already done)
git clone https://github.com/Ali-Maq/Final_Dataset_Civic.git
cd Final_Dataset_Civic

# Install Qwen-Agent dependencies
pip install -r requirements_qwen_agent.txt
```

### Configuration

Choose one of the following deployment options:

#### Option 1: DashScope (Cloud - Recommended for Getting Started)

```bash
# Get API key from: https://dashscope.aliyun.com/
export DASHSCOPE_API_KEY="your_dashscope_api_key"

# Run demo
python demos/demo_oncocite_qwen.py
```

#### Option 2: Local Deployment with vLLM

```bash
# Install vLLM
pip install vllm

# Start vLLM server
vllm serve Qwen/Qwen2.5-7B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --max-model-len 32768

# In another terminal, configure and run
export ONCOCITE_QWEN_MODEL_SERVER="http://localhost:8000/v1"
python demos/demo_oncocite_qwen.py
```

## üìö Documentation

- **[Migration Guide](docs/QWEN_AGENT_MIGRATION.md)**: Comprehensive guide for migrating from OpenAI Agents SDK
- **[Configuration](config/config_oncocite.py)**: Configuration options for both frameworks
- **[Demo Script](demos/demo_oncocite_qwen.py)**: Example usage with sample literature

## üèóÔ∏è Implementation Overview

### File Structure

```
Final_Dataset_Civic/
‚îú‚îÄ‚îÄ src/agents/
‚îÇ   ‚îú‚îÄ‚îÄ oncocite_agents.py          # Original (OpenAI Agents SDK)
‚îÇ   ‚îî‚îÄ‚îÄ oncocite_agents_qwen.py     # New (Qwen-Agent Framework) ‚≠ê
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config_oncocite.py          # Updated with Qwen support
‚îú‚îÄ‚îÄ demos/
‚îÇ   ‚îú‚îÄ‚îÄ demo_oncocite.py            # Original demo
‚îÇ   ‚îî‚îÄ‚îÄ demo_oncocite_qwen.py       # New Qwen demo ‚≠ê
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ QWEN_AGENT_MIGRATION.md     # Comprehensive migration guide ‚≠ê
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_qwen_agent_structure.py # Structure tests ‚≠ê
‚îú‚îÄ‚îÄ requirements_qwen_agent.txt      # Qwen-Agent dependencies ‚≠ê
‚îî‚îÄ‚îÄ README_QWEN_AGENT.md            # This file ‚≠ê
```

### Key Components

1. **OncoCITEOrchestrator**: Main orchestration class for the 18-agent pipeline
2. **OncoCITEExtractionAgent**: Base agent class implementing Qwen-Agent's Agent interface
3. **Tier 1 Agents (1-8)**: Extraction agents for core entities
4. **Tier 2 Agents (9-14)**: Normalization agents for ontology mapping
5. **Tier 3 Agents (15-17)**: Validation agents for quality assurance
6. **Tier 4 Agent (18)**: Consolidation agent for final output

## üîß Usage Examples

### Basic Usage

```python
from agents.oncocite_agents_qwen import OncoCITEOrchestrator
from config.config_oncocite import OncoCITEConfig

# Load configuration
config = OncoCITEConfig.from_env()
llm_cfg = config.get_qwen_llm_config()

# Initialize orchestrator
orchestrator = OncoCITEOrchestrator(llm_cfg=llm_cfg, verbose=True)

# Process literature
literature_text = """
    [Your oncology literature text here]
"""

result = orchestrator.process_literature(literature_text)

# Access structured output
print(f"Disease: {result.disease_name}")
print(f"Variant: {result.variant_names}")
print(f"Evidence Level: {result.evidence_level}")
```

### Custom Model Configuration

```python
# Using DashScope
llm_cfg = {
    'model': 'qwen-max-latest',
    'model_type': 'qwen_dashscope',
    'api_key': 'your_api_key',
    'generate_cfg': {
        'max_input_tokens': 58000,
        'temperature': 0.7
    }
}

# Using local vLLM
llm_cfg = {
    'model': 'Qwen2.5-32B-Instruct',
    'model_server': 'http://localhost:8000/v1',
    'api_key': 'EMPTY',
    'generate_cfg': {
        'max_input_tokens': 32768,
    }
}

orchestrator = OncoCITEOrchestrator(llm_cfg=llm_cfg)
```

## üîÑ Migration from OpenAI Agents SDK

### Before (OpenAI)

```python
from agents.oncocite_agents import OncoCITEOrchestrator

orchestrator = OncoCITEOrchestrator(verbose=True)
result = await orchestrator.process_literature(text)
```

### After (Qwen)

```python
from agents.oncocite_agents_qwen import OncoCITEOrchestrator
from config.config_oncocite import OncoCITEConfig

config = OncoCITEConfig.from_env()
llm_cfg = config.get_qwen_llm_config()
orchestrator = OncoCITEOrchestrator(llm_cfg=llm_cfg, verbose=True)
result = orchestrator.process_literature(text)  # No await
```

**Key Changes:**
- Import from `oncocite_agents_qwen` instead of `oncocite_agents`
- Pass `llm_cfg` to orchestrator
- Synchronous execution (no `async`/`await`)

## üí° Why Qwen-Agent?

### Advantages

‚úÖ **Cost Savings**: Up to 10x cheaper than OpenAI GPT-4
‚úÖ **Data Privacy**: Deploy locally for sensitive medical data
‚úÖ **Flexibility**: Multiple deployment options (cloud, on-prem, edge)
‚úÖ **Performance**: Comparable quality to GPT-4 level models
‚úÖ **Open Source**: Full transparency and customization

### Model Recommendations

| Model | Use Case | Performance | Deployment |
|-------|----------|-------------|------------|
| Qwen-Max | Production (cloud) | Excellent | DashScope |
| Qwen2.5-32B | Production (on-prem) | Excellent | vLLM |
| Qwen2.5-14B | Development | Good | vLLM/Ollama |
| Qwen2.5-7B | Prototyping | Fair | Ollama |

## üöÄ Deployment Options

### 1. DashScope (Cloud Service)

**Best for:** Getting started, prototyping, low-volume production

```bash
export DASHSCOPE_API_KEY="your_key"
export ONCOCITE_QWEN_MODEL="qwen-max-latest"
python demos/demo_oncocite_qwen.py
```

**Pros:**
- No infrastructure setup
- Always latest models
- Pay-per-use pricing

**Cons:**
- Requires internet
- Data sent to cloud
- Usage-based costs

### 2. vLLM (High-Performance Local)

**Best for:** High-throughput production, data privacy requirements

```bash
# Start vLLM server
vllm serve Qwen/Qwen2.5-32B-Instruct \
    --host 0.0.0.0 \
    --port 8000 \
    --tensor-parallel-size 2 \
    --max-model-len 32768

# Configure OncoCITE
export ONCOCITE_QWEN_MODEL_SERVER="http://localhost:8000/v1"
export ONCOCITE_QWEN_MODEL_LOCAL="Qwen2.5-32B-Instruct"
```

**Hardware Requirements:**
- Qwen2.5-7B: 1x GPU (16GB VRAM)
- Qwen2.5-32B: 2x GPU (24GB VRAM each)

**Pros:**
- Highest throughput
- Full data privacy
- Cost-effective at scale

**Cons:**
- Requires GPU infrastructure
- More complex setup

### 3. Ollama (Easy Local)

**Best for:** Development, testing, CPU-only environments

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull model
ollama pull qwen2.5:7b

# Configure OncoCITE
export ONCOCITE_QWEN_MODEL_SERVER="http://localhost:11434/v1"
```

**Pros:**
- Easy setup
- Works on CPU
- Good for development

**Cons:**
- Lower throughput
- Limited to smaller models

## üî¨ Testing

### Structure Tests

```bash
# Install dependencies first
pip install -r requirements_qwen_agent.txt

# Run structure tests (no API key needed)
python tests/test_qwen_agent_structure.py
```

### Full Integration Test

```bash
# Requires API key or local model server
export DASHSCOPE_API_KEY="your_key"
python demos/demo_oncocite_qwen.py
```

## üìä Performance Comparison

### OpenAI vs Qwen

| Metric | OpenAI (GPT-4o) | Qwen-Max | Qwen2.5-32B (vLLM) |
|--------|----------------|----------|---------------------|
| Quality | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Speed | ~30-60s | ~30-60s | ~20-40s |
| Cost/1M tokens | $15 | $1.50 | $0 (hardware) |
| Privacy | Cloud | Cloud/Local | Local |
| Customization | Limited | Limited | Full |

### Processing Times

For a typical oncology abstract (2000 tokens) through the full 18-agent pipeline:

- **DashScope**: ~30-60 seconds
- **vLLM (32B, 2xGPU)**: ~20-40 seconds
- **Ollama (7B, CPU)**: ~2-5 minutes

## üêõ Troubleshooting

### Common Issues

**1. ModuleNotFoundError: No module named 'qwen_agent'**

```bash
pip install qwen-agent>=0.0.31
```

**2. DASHSCOPE_API_KEY not found**

```bash
export DASHSCOPE_API_KEY="your_key"
# Add to ~/.bashrc for persistence
echo 'export DASHSCOPE_API_KEY="your_key"' >> ~/.bashrc
```

**3. Connection refused (local deployment)**

Check if server is running:
```bash
curl http://localhost:8000/v1/models  # vLLM
curl http://localhost:11434/v1/models # Ollama
```

**4. Out of memory**

Use a smaller model:
```bash
vllm serve Qwen/Qwen2.5-7B-Instruct ...
# or
ollama pull qwen2.5:3b
```

## üìñ Additional Resources

### Qwen-Agent Documentation

- [Qwen-Agent GitHub](https://github.com/QwenLM/Qwen-Agent)
- [Qwen Models](https://github.com/QwenLM/Qwen)
- [DashScope API](https://help.aliyun.com/zh/dashscope/)

### Deployment Guides

- [vLLM Documentation](https://docs.vllm.ai/)
- [Ollama Documentation](https://ollama.ai/docs)
- [Qwen Model Cards](https://huggingface.co/Qwen)

### OncoCITE Documentation

- [Complete Agent Documentation](docs/AGENT_DETAILS_COMPLETE.md)
- [Migration Guide](docs/QWEN_AGENT_MIGRATION.md)
- [System Architecture](README.md)

## ü§ù Contributing

Contributions are welcome! Areas for improvement:

1. **Parallel Execution**: Add asyncio support for Tier 1 agents
2. **Streaming**: Real-time progress updates
3. **Fine-tuning**: Custom Qwen models for oncology
4. **RAG**: Long-context document processing
5. **MCP Tools**: Medical knowledge base integration

## üìù License

This project maintains the same license as the original OncoCITE system. Please refer to the main LICENSE file.

## üìß Support

For questions or issues:

1. Check the [Migration Guide](docs/QWEN_AGENT_MIGRATION.md)
2. Review [example code](demos/demo_oncocite_qwen.py)
3. Open an issue on GitHub
4. Contact the OncoCITE development team

---

**Last Updated**: 2025-01-11
**Version**: 1.0.0
**Framework**: Qwen-Agent 0.0.31+
**Compatible Models**: Qwen2.5, Qwen-Max, Qwen3
